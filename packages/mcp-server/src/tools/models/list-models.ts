// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { maybeFilter } from 'dedalus-labs-mcp/filtering';
import { Metadata, asTextContentResult } from 'dedalus-labs-mcp/tools/types';

import { Tool } from '@modelcontextprotocol/sdk/types.js';
import Dedalus from 'dedalus-labs';

export const metadata: Metadata = {
  resource: 'models',
  operation: 'read',
  tags: [],
  httpMethod: 'get',
  httpPath: '/v1/models',
  operationId: 'list_models_v1_models_get',
};

export const tool: Tool = {
  name: 'list_models',
  description:
    "When using this tool, always use the `jq_filter` parameter to reduce the response size and improve performance.\n\nOnly omit if you're sure you don't need the data.\n\nList available models.\n\nReturns a list of available models from all configured providers.\nModels are filtered based on provider availability and API key configuration.\nOnly models from providers with valid API keys are returned.\n\nArgs:\n    user: Authenticated user obtained from API key validation\n\nReturns:\n    ModelsResponse: Object containing list of available models\n\nRaises:\n    HTTPException:\n        - 401 if authentication fails\n        - 500 if internal error occurs during model listing\n\nRequires:\n    Valid API key with 'read' scope permission\n\nExample:\n    ```python\n    import dedalus_labs\n\n    client = dedalus_labs.Client(api_key=\"your-api-key\")\n    models = client.models.list()\n\n    for model in models.data:\n        print(f\"Model: {model.id} (Owner: {model.owned_by})\")\n    ```\n\n    Response:\n    ```json\n    {\n        \"object\": \"list\",\n        \"data\": [\n            {\n                \"id\": \"openai/gpt-4\",\n                \"object\": \"model\",\n                \"owned_by\": \"openai\"\n            },\n            {\n                \"id\": \"anthropic/claude-3-5-sonnet-20241022\",\n                \"object\": \"model\",\n                \"owned_by\": \"anthropic\"\n            }\n        ]\n    }\n    ```\n\n# Response Schema\n```json\n{\n  $ref: '#/$defs/models_response',\n  $defs: {\n    models_response: {\n      type: 'object',\n      title: 'ModelsResponse',\n      description: 'Response containing list of available models.\\n\\nReturns all models available to the authenticated user based on\\ntheir API key permissions and configured providers.\\n\\nAttributes:\\n    object: Always \\'list\\' for compatibility with OpenAI API\\n    data: list of Model objects representing available models\\n\\nExample:\\n    {\\n        \"object\": \"list\",\\n        \"data\": [\\n            {\\n                \"id\": \"openai/gpt-4\",\\n                \"object\": \"model\",\\n                \"owned_by\": \"openai\"\\n            },\\n            {\\n                \"id\": \"anthropic/claude-3-5-sonnet-20241022\",\\n                \"object\": \"model\",\\n                \"owned_by\": \"anthropic\"\\n            }\\n        ]\\n    }',\n      properties: {\n        data: {\n          type: 'array',\n          title: 'Data',\n          description: 'List of models',\n          items: {\n            $ref: '#/$defs/model'\n          }\n        },\n        object: {\n          type: 'string',\n          title: 'Object',\n          description: 'Object type'\n        }\n      },\n      required: [        'data'\n      ]\n    },\n    model: {\n      type: 'object',\n      title: 'DedalusModel',\n      description: 'Model information and configuration for Dedalus API.\\n\\nRepresents both model metadata (for listings) and configuration options\\n(for chat completions). When used in GET /v1/models, only metadata fields\\nare populated. When used in chat requests, configuration fields control\\nmodel behavior.\\n\\nAttributes:\\n    id: Provider-prefixed model identifier (e.g., \\'openai/gpt-4\\', \\'anthropic/claude-3-5-sonnet\\')\\n    name: Alias for id (used in chat requests)\\n    object: Always \\'model\\' for OpenAI compatibility\\n    created: Unix timestamp when model was created\\n    owned_by: Provider organization that owns the model\\n    root: Base model identifier if this is a fine-tuned variant\\n    parent: Parent model identifier for hierarchical relationships\\n    permission: Access permissions (reserved for future use)\\n\\n    Configuration fields (used in chat requests):\\n    temperature: Sampling temperature (0 to 2)\\n    top_p: Nucleus sampling parameter (0 to 1)\\n    frequency_penalty: Frequency penalty (-2 to 2)\\n    presence_penalty: Presence penalty (-2 to 2)\\n    max_tokens: Maximum tokens to generate\\n    attributes: Custom attributes for model routing\\n    metadata: Additional metadata\\n\\nExample:\\n    Listing response:\\n    {\\n        \"id\": \"openai/gpt-4o\",\\n        \"object\": \"model\",\\n        \"created\": 1687882411,\\n        \"owned_by\": \"openai\"\\n    }\\n\\n    Chat request:\\n    {\\n        \"name\": \"openai/gpt-4o\",\\n        \"temperature\": 0.7,\\n        \"max_tokens\": 1000,\\n        \"attributes\": {\"intelligence\": 0.9, \"cost\": 0.8}\\n    }',\n      properties: {\n        id: {\n          type: 'string',\n          title: 'Id',\n          description: 'Model identifier'\n        },\n        attributes: {\n          type: 'object',\n          title: 'Attributes',\n          description: '[Dedalus] Custom attributes for intelligent model routing (e.g., intelligence, speed, creativity, cost).',\n          additionalProperties: true\n        },\n        created: {\n          type: 'integer',\n          title: 'Created',\n          description: 'Creation timestamp'\n        },\n        frequency_penalty: {\n          type: 'number',\n          title: 'Frequency Penalty',\n          description: 'Penalize new tokens based on their frequency in the text so far.'\n        },\n        logit_bias: {\n          type: 'object',\n          title: 'Logit Bias',\n          description: 'Modify the likelihood of specified tokens appearing.',\n          additionalProperties: true\n        },\n        logprobs: {\n          type: 'boolean',\n          title: 'Logprobs',\n          description: 'Whether to return log probabilities of the output tokens.'\n        },\n        max_completion_tokens: {\n          type: 'integer',\n          title: 'Max Completion Tokens',\n          description: 'An upper bound for the number of tokens that can be generated for a completion.'\n        },\n        max_tokens: {\n          type: 'integer',\n          title: 'Max Tokens',\n          description: 'Maximum number of tokens to generate.'\n        },\n        metadata: {\n          type: 'object',\n          title: 'Metadata',\n          description: '[Dedalus] Additional metadata for request tracking and debugging.',\n          additionalProperties: true\n        },\n        n: {\n          type: 'integer',\n          title: 'N',\n          description: 'Number of completions to generate for each prompt.'\n        },\n        name: {\n          type: 'string',\n          title: 'Name',\n          description: 'Model name (alias for id in chat requests)'\n        },\n        object: {\n          type: 'string',\n          title: 'Object',\n          description: 'Object type'\n        },\n        owned_by: {\n          type: 'string',\n          title: 'Owned By',\n          description: 'Model owner'\n        },\n        parallel_tool_calls: {\n          type: 'boolean',\n          title: 'Parallel Tool Calls',\n          description: 'Whether to enable parallel function calling.'\n        },\n        parent: {\n          type: 'string',\n          title: 'Parent',\n          description: 'Parent model'\n        },\n        permission: {\n          type: 'array',\n          title: 'Permission',\n          description: 'Permissions',\n          items: {\n            type: 'object',\n            additionalProperties: true\n          }\n        },\n        presence_penalty: {\n          type: 'number',\n          title: 'Presence Penalty',\n          description: 'Penalize new tokens based on whether they appear in the text so far.'\n        },\n        response_format: {\n          type: 'object',\n          title: 'Response Format',\n          description: 'Format for the model output (e.g., {\\'type\\': \\'json_object\\'}).',\n          additionalProperties: true\n        },\n        root: {\n          type: 'string',\n          title: 'Root',\n          description: 'Root model'\n        },\n        seed: {\n          type: 'integer',\n          title: 'Seed',\n          description: 'Seed for deterministic sampling.'\n        },\n        service_tier: {\n          type: 'string',\n          title: 'Service Tier',\n          description: 'Latency tier for the request (e.g., \\'auto\\', \\'default\\').'\n        },\n        stop: {\n          anyOf: [            {\n              type: 'string'\n            },\n            {\n              type: 'array',\n              items: {\n                type: 'string'\n              }\n            }\n          ],\n          title: 'Stop',\n          description: 'Up to 4 sequences where the API will stop generating further tokens.'\n        },\n        stream: {\n          type: 'boolean',\n          title: 'Stream',\n          description: 'Whether to stream back partial progress.'\n        },\n        stream_options: {\n          type: 'object',\n          title: 'Stream Options',\n          description: 'Options for streaming responses.',\n          additionalProperties: true\n        },\n        temperature: {\n          type: 'number',\n          title: 'Temperature',\n          description: 'Sampling temperature (0 to 2). Higher values make output more random.'\n        },\n        tool_choice: {\n          anyOf: [            {\n              type: 'string'\n            },\n            {\n              type: 'object',\n              additionalProperties: true\n            }\n          ],\n          title: 'Tool Choice',\n          description: 'Controls which tool is called by the model.'\n        },\n        tools: {\n          type: 'array',\n          title: 'Tools',\n          description: 'List of tools the model may call.',\n          items: {\n            type: 'object',\n            additionalProperties: true\n          }\n        },\n        top_logprobs: {\n          type: 'integer',\n          title: 'Top Logprobs',\n          description: 'Number of most likely tokens to return at each token position.'\n        },\n        top_p: {\n          type: 'number',\n          title: 'Top P',\n          description: 'Nucleus sampling parameter. Alternative to temperature.'\n        },\n        user: {\n          type: 'string',\n          title: 'User',\n          description: 'A unique identifier representing your end-user.'\n        }\n      }\n    }\n  }\n}\n```",
  inputSchema: {
    type: 'object',
    properties: {
      jq_filter: {
        type: 'string',
        title: 'jq Filter',
        description:
          'A jq filter to apply to the response to include certain fields. Consult the output schema in the tool description to see the fields that are available.\n\nFor example: to include only the `name` field in every object of a results array, you can provide ".results[].name".\n\nFor more information, see the [jq documentation](https://jqlang.org/manual/).',
      },
    },
    required: [],
  },
  annotations: {
    readOnlyHint: true,
  },
};

export const handler = async (client: Dedalus, args: Record<string, unknown> | undefined) => {
  const { jq_filter } = args as any;
  return asTextContentResult(await maybeFilter(jq_filter, await client.models.list()));
};

export default { metadata, tool, handler };
